<p align="center"><img width="50%" src="logo/Deeplearningwithtensorflow20c.png" /></p>

<h1 align="center">Deep Learning with Tensorflow 2.0</h1>



<p>
    This is the Jupyter notebook version of the Deep Learning with Tensorflow 2.0 by Mukesh Mithrakumar; the content
    is available <a href="https://github.com/mukeshmithrakumar/DeepLearningWithTF2.0">on GitHub</a> and you can run
    it in Google Colaboratory as well, the link is provided inside the Jupyter notebook.
    The code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>.<br>
</p>

<p>
    If you find this content useful, consider buying me a cup of coffee ‚òïÔ∏èüòâ. <br>
</p>
<div style="padding-top:10px;">
    <span class="badge-buymeacoffee"><a href="https://buymeacoffee.com/mmukesh"
            title="Donate to this project using Buy Me A Coffee"><img
                src="https://img.shields.io/badge/buy%20me%20a%20coffee-donate-blue.svg"
                alt="Buy Me A Coffee donate button" /></a></span>
    <span class="badge-paypal"><a href="https://paypal.me/mukeshmithrakumar"
            title="Donate to this project using Paypal"><img src="https://img.shields.io/badge/paypal-donate-green.svg"
                alt="PayPal donate button" /></a></span>
</div>


<h2 style="padding-top:200px;">Table of Contents</h2>

<a href="00.00-Preface.ipynb">
    <h2>0. Preface</h2>
</a>

<a href="01.00-Introduction.ipynb">
    <h2>1. Introduction (05.05.2019)</h2>
</a>
<li>01.01 Who should read this book</li>
<li>01.02 Historical Trends in Deep Learning</li>

<a href="02.00-Linear-Algebra.ipynb">
    <h2>2. Applied Math and Machine Learning Basics (05.12.2019)</h2>
</a>
<li>02.01 Scalars, Vectors, Matrices and Tensors</li>
<li>02.02 Multiplying Matrices and Vectors</li>
<li>02.03 Identity and Inverse Matrices</li>
<li>02.04 Linear Dependence and Span</li>
<li>02.05 Norms</li>
<li>02.06 Special Kinds of Matrices and Vectors</li>
<li>02.07 Eigendecomposition</li>
<li>02.08 Singular Value Decomposition</li>
<li>02.09 The Moore-Penrose Pseudoinverse</li>
<li>02.10 The Trace Operator</li>
<li>02.11 The Determinant</li>
<li>02.12 Example: Principal Components Analysis</li>


<a href="03.00-Probability-and-Information-Theory.ipynb">
    <h2>3. Probability and Information Theory (05.19.2019)</h2>
</a>
<li>03.01 Why Probability?</li>
<li>03.02 Random Variables</li>
<li>03.03 Probability Distributions</li>
<li>03.04 Marginal Probability</li>
<li>03.05 Conditional Probability</li>
<li>03.06 The Chain Rule of Conditional Probabilities</li>
<li>03.07 Independence and Conditional Independence</li>
<li>03.08 Expectation, Variance and Covariance</li>
<li>03.09 Common Probability Distributions</li>
<li>03.10 Useful Properties of Common Functions</li>
<li>03.11 Bayes' Rule</li>
<li>03.12 Technical Details of Continuous Variables</li>
<li>03.13 Information Theory</li>
<li>03.14 Structured Probabilistic Models</li>


<a href="04.00-Numerical-Computation.ipynb">
    <h2>4. Numerical Computation (05.26.2019)</h2>
</a>
<li>04.01 Overflow and Underflow</li>
<li>04.02 Poor Conditioning</li>
<li>04.03 Gradient-Based Optimization</li>
<li>04.04 Constrained Optimization</li>
<li>04.05 Example: Linear Least Squares</li>


<a href="05.00-Machine-Learning-Basics.ipynb">
    <h2>5. Machine Learning Basics (06.02.2019)</h2>
</a>
<li>05.01 Learning Algorithms</li>
<li>05.02 Capacity, Overfitting and Underfitting</li>
<li>05.03 Hyperparameters and Validation Sets</li>
<li>05.04 Estimators, Bias and Variance</li>
<li>05.05 Maximum Likelihood Estimation</li>
<li>05.06 Bayesian Statistics</li>
<li>05.07 Supervised Learning Algorithms</li>
<li>05.08 Unsupervised Learning Algorithms</li>
<li>05.09 Stochastic Gradient Descent</li>
<li>05.10 Building a Machine Learning Algorithm</li>
<li>05.11 Challenges Motivating Deep Learning</li>


<a href="06.00-Deep-Feedforward-Networks.ipynb">
    <h2>6. Deep Feedforward Networks (06.09.2019)</h2>
</a>
<li>06.01 Example: Learning XOR</li>
<li>06.02 Gradient-Based Learning</li>
<li>06.03 Hidden Units</li>
<li>06.04 Architecture Design</li>
<li>06.05 Back-Propagation and Other Differentiation Algorithms</li>
<li>06.06 Historical Notes</li>


<a href="07.00-Regularization-for-Deep-Learning.ipynb">
    <h2>7. Regularization for Deep Learning (06.16.2019)</h2>
</a>
<li>07.01 Parameter Norm Penalties</li>
<li>07.02 Norm Penalties as Constrained Optimization</li>
<li>07.03 Regularization and Under-Constrained Problems</li>
<li>07.04 Dataset Augmentation</li>
<li>07.05 Noise Robustness</li>
<li>07.06 Semi-Supervised Learning</li>
<li>07.07 Multitask Learning</li>
<li>07.08 Early Stopping</li>
<li>07.09 Parameter Tying and Parameter Sharing</li>
<li>07.10 Sparse Representations</li>
<li>07.11 Bagging and Other Ensemble Methods</li>
<li>07.12 Dropout</li>
<li>07.13 Adversarial Training</li>
<li>07.14 Tangent Distance, Tangent Prop and Manifold Tangent Classifier</li>


<a href="08.00-Optimization-for-Training-Deep-Models.ipynb">
    <h2>8. Optimization for Training Deep Models (06.23.2019)</h2>
</a>
<li>08.01 How Learning Differs from Pure Optimization</li>
<li>08.02 Challenges in Neural Network Optimization</li>
<li>08.03 Basic Algorithms</li>
<li>08.04 Parameter Initialization Strategies</li>
<li>08.05 Algorithms with Adaptive Learning Rates</li>
<li>08.06 Approximate Second-Order Methods</li>
<li>08.07 Optimization Strategies and Meta-Algorithms</li>


<a href="09.00-Convolutional-Networks.ipynb">
    <h2>9. Convolutional Networks (06.30.2019)</h2>
</a>
<li>09.01 The Convolution Operation</li>
<li>09.02 Motivation</li>
<li>09.03 Pooling</li>
<li>09.04 Convolution and Pooling as an Infinitely Strong Prior</li>
<li>09.05 Variants of the Basic Convolution Function</li>
<li>09.06 Structured Outputs</li>
<li>09.07 Data Types</li>
<li>09.08 Efficient Convolution Algorithms</li>
<li>09.09 Random or Unsupervised Features</li>
<li>09.10 The Neuroscientific Basis for Convolutional Networks</li>
<li>09.11 Convolutional Networks and the History of Deep Learning</li>


<a href="10.00-Sequence-Modeling-Recurrent-and-Recursive-Nets.ipynb">
    <h2>10. Sequence Modeling: Recurrent and Recursive Nets (07.07.2019)</h2>
</a>
<li>10.01 Unfolding Computational Graphs</li>
<li>10.02 Recurrent Neural Networks</li>
<li>10.03 Bidirectional RNNs</li>
<li>10.04 Encoder-Decoder Sequence-to-Sequence Architectures</li>
<li>10.05 Deep Recurrent Networks</li>
<li>10.06 Recursive Neural Networks</li>
<li>10.07 The Challenge of Long-Term Dependencies</li>
<li>10.08 Echo State Networks</li>
<li>10.09 Leaky Units and Other Strategies for Multiple Time Scales</li>
<li>10.10 The Long Short-Term Memory and Other Gated RNNs</li>
<li>10.11 Optimization for Long-Term Dependencies</li>
<li>10.12 Explicit Memory</li>


<a href="11.00-Practical-Methodology.ipynb">
    <h2>11. Practical Methodology (07.14.2019)</h2>
</a>
<li>11.01 Performance Metrics</li>
<li>11.02 Default Baseline Models</li>
<li>11.03 Determining Whether to Gather More Data</li>
<li>11.04 Selecting Hyperparameters</li>
<li>11.05 Debugging Strategies</li>
<li>11.06 Example: Multi-Digit Number Recognition</li>


<a href="12.00-Applications.ipynb">
    <h2>12. Applications (07.21.2019)</h2>
</a>
<li>12.01 Large-Scale Deep Learning</li>
<li>12.02 Computer Vision</li>
<li>12.03 Speech Recognition</li>
<li>12.04 Natural Language Processing</li>
<li>12.05 Other Applications</li>


<a href="13.00-Linear-Factor-Models.ipynb">
    <h2>13. Linear Factor Models (07.28.2019)</h2>
</a>
<li>13.01 Probabilistic PCA and Factor Analysis</li>
<li>13.02 Independent Component Analysis</li>
<li>13.03 Slow Feature Analysis</li>
<li>13.04 Sparse Coding</li>
<li>13.05 Manifold Interpretation of PCA</li>


<a href="14.00-Autoencoders.ipynb">
    <h2>14. Autoencoders (08.04.2019)</h2>
</a>
<li>14.01 Undercomplete Autoencoders</li>
<li>14.02 Regularized Autoencoders</li>
<li>14.03 Representational Power, Layer Size and Depth</li>
<li>14.04 Stochastic Encoders and Decoders</li>
<li>14.05 Denoising Autoencoders</li>
<li>14.06 Learning Manifolds with Autoencoders</li>
<li>14.07 Contractive Autoencoders</li>
<li>14.08 Predictive Sparse Decomposition</li>
<li>14.09 Applications of Autoencoders</li>


<a href="15.00-Representation-Learning.ipynb">
    <h2>15. Representation Learning (08.11.2019)</h2>
</a>
<li>15.01 Greedy Layer-Wise Unsupervised Pretraining</li>
<li>15.02 Transfer Learning and Domain Adaptation</li>
<li>15.03 Semi-Supervised Disentangling of Causal Factors</li>
<li>15.04 Distributed Representation</li>
<li>15.05 Exponential Gains from Depth</li>
<li>15.06 Providing Clues to Discover Underlying Causes</li>


<a href="16.00-Structured-Probabilistic-Models-for-Deep-Learning.ipynb">
    <h2>16. Structured Probabilistic Models for Deep Learning (08.18.2019)</h2>
</a>
<li>16.01 The Challenge of Unstructured Modeling</li>
<li>16.02 Using Graphs to Describe Model Structure</li>
<li>16.03 Sampling from Graphical Models</li>
<li>16.04 Advantages of Structured Modeling</li>
<li>16.05 Learning about Dependencies</li>
<li>16.06 Inference and Approximate Inference</li>
<li>16.07 The Deep Learning Approach to Structured Probabilistic Models</li>


<a href="17.00-Monte-Carlo-Methods.ipynb">
    <h2>17. Monte Carlo Methods (08.25.2019)</h2>
</a>
<li>17.01 Sampling and Monte Carlo Methods</li>
<li>17.02 Importance Sampling</li>
<li>17.03 Markov Chain Monte Carlo Methods</li>
<li>17.04 Gibbs Sampling</li>
<li>17.05 The Challenge of Mixing between Separated Modes</li>


<a href="18.00-Confronting-the-Partition-Function.ipynb">
    <h2>18. Confronting the Partition Function (09.01.2019)</h2>
</a>
<li>18.01 The Log-Likelihood Gradient</li>
<li>18.02 Stochastic Maximum Likelihood and Contrastive Divergence</li>
<li>18.03 Pseudolikelihood</li>
<li>18.04 Score Matching and Ratio Matching</li>
<li>18.05 Denoising Score Matching</li>
<li>18.06 Noise-Contrastive Estimation</li>
<li>18.07 Estimating the Partition Function</li>


<a href="19.00-Approximate-Inference.ipynb">
    <h2>19. Approximate Inference (09.08.2019)</h2>
</a>
<li>19.01 Inference as Optimization</li>
<li>19.02 Expectation Maximization</li>
<li>19.03 MAP Inference and Sparse Coding</li>
<li>19.04 Variational Inference and Learning</li>
<li>19.05 Learned Approximate Inference</li>


<a href="20.00-Deep-Generative-Models.ipynb">
    <h2>20. Deep Generative Models (09.15.2019)</h2>
</a>
<li>20.01 Boltzmann Machines</li>
<li>20.02 Restricted Boltzmann Machines</li>
<li>20.03 Deep Belief Networks</li>
<li>20.04 Deep Boltzmann Machines</li>
<li>20.05 Boltzmann Machines for Real-Valued Data</li>
<li>20.06 Convolutional Boltzmann Machines</li>
<li>20.07 Boltzmann Machines for Structured or Sequential Outputs</li>
<li>20.08 Other Boltzmann Machines</li>
<li>20.09 Back-Propagation through Random Operations</li>
<li>20.10 Directed Generative Nets</li>
<li>20.11 Drawing Samples from Autoencoders</li>
<li>20.12 Generative Stochastic Networks</li>
<li>20.13 Other Generation Schemes</li>
<li>20.14 Evaluating Generative Models</li>
<li>20.15 Conclusion</li>
